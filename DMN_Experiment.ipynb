{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMN Experiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7FTRiNIOCSh"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"EXPERIMENT_15_TASK_11_DROPOUT_0.9\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at https://colab.research.google.com/drive/1t2ym5S65ZUurXSAfPEIbZvVTTnKCluY\n",
        "Z \"\"\"\n",
        "# %matplotlib inline\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt import matplotlib.ticker as ticker import urllib\n",
        "import sys\n",
        "import os\n",
        "import hashlib\n",
        "import re\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "#connecting the Google drive\n",
        "from google.colab import drive drive.mount('/content/drive')\n",
        "train_set_post_file = '/content/drive/My Drive/GloVe/en-10k/qa11_basic- coreference_train.txt'\n",
        "test_set_post_file = '/content/drive/My Drive/GloVe/en-10k/qa11_basic- coreference_test.txt'\n",
        "glove_vectors_file = '/content/drive/My Drive/GloVe/glove.6B.50d.txt'\n",
        "# Prepare glove_wordmap\n",
        "glove_wordmap = {}\n",
        "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
        "for line in glove:\n",
        "name, vector = tuple(line.split(\" \", 1)) glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n",
        "wvecs = []\n",
        "for item in glove_wordmap.items():\n",
        "wvecs.append(item[1]) s = np.vstack(wvecs)\n",
        "v = np.var(s,0)\n",
        "m = np.mean(s,0)\n",
        "RS = np.random.RandomState()\n",
        "def fill_unk(unk):\n",
        "global glove_wordmap\n",
        "glove_wordmap[unk] = RS.multivariate_normal(m,np.diag(v)) return glove_wordmap[unk]\n",
        "def sentence2sequence(sentence):\n",
        "#This is where the tokenization of words happens. The sentences are stripped\n",
        "63\n",
        "#and splitted on spaces and any brackets, commas and hypens are stripped.\n",
        "#This function will be used inside of the contextualise functinon.\n",
        "tokens = sentence.strip('\"(),-').lower().split(\" \") rows = []\n",
        "words = []\n",
        "for token in tokens: i = len(token)\n",
        "while len(token) > 0:\n",
        "word = token[:i]\n",
        "if word in glove_wordmap:\n",
        "rows.append(glove_wordmap[word]) #appending happens with the glove vector #This is where word embeddings happen\n",
        "  words.append(word) token = token[i:] i = len(token) continue\n",
        "else:\n",
        "i = i-1\n",
        "if i == 0:\n",
        "rows.append(fill_unk(token)) words.append(token)\n",
        "break\n",
        "return np.array(rows), words\n",
        "def contextualize(set_file):\n",
        "#This function handles the tokenization part\n",
        "#The split function is used to tokenize at spaces and tab is used to #identify answers and the supporting factors\n",
        "#but we dont use the supporting factors\n",
        "data = []\n",
        "context = []\n",
        "with open(set_file, \"r\", encoding=\"utf8\") as train:\n",
        "for line in train:\n",
        "l, ine = tuple(line.split(\" \", 1))\n",
        "# Split the line numbers from the sentences they refer to. if l is \"1\":\n",
        "# New contexts always start with 1,\n",
        "# so this is a signal to reset the context. context = []\n",
        "if \"\\t\" in ine:\n",
        " # Tabs are the separator between questions and answers, # and are not present in context statements.\n",
        "question, answer, support = tuple(ine.split(\"\\t\")) data.append((tuple(zip(*context))+\n",
        "sentence2sequence(question)+ sentence2sequence(answer)+\n",
        "([int(s) for s in support.split()],)))\n",
        "# Multiple questions may refer to the same context, so we don't reset it.\n",
        "else:\n",
        "# Context sentence. context.append(sentence2sequence(ine[:-1]))\n",
        "return data\n",
        "train_data = contextualize(train_set_post_file) test_data = contextualize(test_set_post_file)\n",
        "final_train_data = []\n",
        "64\n",
        "\n",
        "def finalize(data):\n",
        "#the contextualise function concatonates the sentences #finalize function is where the sentence ends are indexed.\n",
        "final_data = [] for cqas in data:\n",
        "contextvs, contextws, qvs, qws, avs, aws, spt = cqas\n",
        "lengths = itertools.accumulate(len(cvec) for cvec in contextvs) context_vec = np.concatenate(contextvs)\n",
        "context_words = sum(contextws,[])\n",
        "# Location markers for sentence ends/beginnings\n",
        "sentence_ends = np.array(list(lengths))\n",
        "final_data.append((context_vec, sentence_ends, qvs, spt, context_words, cqas, avs, aws))\n",
        "  return np.array(final_data)\n",
        "#execution of the finalise function\n",
        "final_train_data = finalize(train_data) final_test_data = finalize(test_data)\n",
        "tf.reset_default_graph() # Hyperparameters\n",
        "# The number of dimensions used to store data passed between recurrent layers in the network.\n",
        "# the layer size.\n",
        "recurrent_cell_size = 128\n",
        "# The number of dimensions in our word vectorizations. #we use the glove 50D, but we have 100D and 300 aswell. D = 50\n",
        "# How quickly the network learns.\n",
        "learning_rate = 0.005 #regularization\n",
        "input_p, output_p = 0.9, 0.5\n",
        "# How many questions we train on at a time. #increasing this takes up considerable about of GPU\n",
        "batch_size = 128\n",
        " # Number of episodic memory passes\n",
        "passes = 4\n",
        "# The strength of our regularization.\n",
        "weight_decay = 0.00000001\n",
        "# How many questions are trained by the network. This decides the epochs\n",
        "training_iterations_count = 400000\n",
        "# How many iterations of training occur before each validation check. # set it to 79 to check validation accuracy at every epoch. display_step = 79\n",
        "# Input Module\n",
        "# Context: tensor that contains all the context information.\n",
        "65\n",
        "\n",
        "context = tf.placeholder(tf.float32, [None, None, D], \"context\") context_placeholder = context # I use context as a variable name later on\n",
        "# input_sentence_endings: tensor that contains the locations of the ends of sentences.\n",
        "input_sentence_endings = tf.placeholder(tf.int32, [None, None, 2], \"sentence\")\n",
        "# recurrent_cell_size: the number of hidden units in recurrent layers.\n",
        "input_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "#he dropoutwrapper is used to put the dropout to the input module for rwgularization\n",
        "gru_drop = tf.contrib.rnn.DropoutWrapper(input_gru, input_p, output_p)\n",
        "  input_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, context, dtype=tf.float32, scope = \"input_module\")\n",
        "# cs: the facts gathered from the context.\n",
        "cs = tf.gather_nd(input_module_outputs, input_sentence_endings) s = input_module_outputs\n",
        "# Question Module\n",
        "# query: tensor that contains all of the questions.\n",
        "query = tf.placeholder(tf.float32, [None, None, D], \"query\")\n",
        "# input_query_lengths: tensor that contains question length information. # input_query_lengths[:,1] has the actual lengths; input_query_lengths[:,0] is a simple range()\n",
        "input_query_lengths = tf.placeholder(tf.int32, [None, 2], \"query_lengths\")\n",
        "question_module_outputs, _ = tf.nn.dynamic_rnn(gru_drop, query, dtype=tf.float32,\n",
        "scope =\n",
        "tf.VariableScope(True, \"input_module\")) # q: the question states.\n",
        "q = tf.gather_nd(question_module_outputs, input_query_lengths) # Episodic Memory\n",
        " size = tf.stack([tf.constant(1),tf.shape(cs)[1], tf.constant(1)]) re_q = tf.tile(tf.reshape(q,[-1,1,recurrent_cell_size]),size)\n",
        "output_size = 1\n",
        "# Weights and biases\n",
        "attend_init = tf.random_normal_initializer(stddev=0.1)\n",
        "w_1 = tf.get_variable(\"attend_w1\", [1,recurrent_cell_size*7, recurrent_cell_size],\n",
        "tf.float32, initializer = attend_init)\n",
        "w_2 = tf.get_variable(\"attend_w2\", [1,recurrent_cell_size, output_size],\n",
        "tf.float32, initializer = attend_init)\n",
        "b_1 = tf.get_variable(\"attend_b1\", [1, recurrent_cell_size], tf.float32, initializer = attend_init)\n",
        "66\n",
        "\n",
        "b_2 = tf.get_variable(\"attend_b2\", [1, output_size], tf.float32, initializer = attend_init)\n",
        "# Regulate all the weights and biases\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_1)) tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_1)) tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_2)) tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(b_2))\n",
        "#Attention Mechanism\n",
        "  def attention(c, mem, existing_facts):\n",
        "# c: tensor that contains all the facts from the contexts.\n",
        "# mem: tensor that contains the current memory.\n",
        "with tf.variable_scope(\"attending\") as scope:\n",
        "# in this function the context and the current memory is\n",
        "concatonated.\n",
        "attending = tf.concat([c, mem, re_q, c * re_q, c * mem, (c- re_q)**2, (c-mem)**2], 2)\n",
        "# m1: First layer of multiplied weights for the feed-forward network.\n",
        "m1 = tf.matmul(attending * existing_facts, tf.tile(w_1,\n",
        "tf.stack([tf.shape(attending)[0],1,1]))) * existing_facts\n",
        "# bias_1: A masked version of the first feed-forward layer's\n",
        "bias\n",
        "# over only existing facts.\n",
        "bias_1 = b_1 * existing_facts\n",
        "tnhan = tf.nn.relu(m1 + bias_1)\n",
        "# m2: Second layer of multiplied weights for the feed-forward network.\n",
        "m2 = tf.matmul(tnhan, tf.tile(w_2, tf.stack([tf.shape(attending)[0],1,1])))\n",
        " bias.\n",
        "# bias_2: A masked version of the second feed-forward layer's\n",
        "bias_2 = b_2 * existing_facts\n",
        "norm_m2 = tf.nn.l2_normalize(m2 + bias_2, -1)\n",
        "softmax_idx = tf.where(tf.not_equal(norm_m2, 0))[:,:-1] softmax_gather = tf.gather_nd(norm_m2[...,0], softmax_idx) softmax_shape = tf.shape(norm_m2, out_type=tf.int64)[:-1] softmaxable = tf.SparseTensor(softmax_idx, softmax_gather,\n",
        "softmax_shape) return\n",
        "tf.expand_dims(tf.sparse_tensor_to_dense(tf.sparse_softmax(softmaxable)) ,-1)\n",
        "67\n",
        "\n",
        "# facts_0s: tensor whose values are 1 if the corresponding fact exists and 0 if not.\n",
        "facts_0s = tf.cast(tf.count_nonzero(input_sentence_endings[:,:,-1:],- 1,keep_dims=True),tf.float32)\n",
        "with tf.variable_scope(\"Episodes\") as scope:\n",
        "attention_gru = tf.contrib.rnn.GRUCell(recurrent_cell_size)\n",
        "# memory: A list of all tensors that are the (current or past) memory state\n",
        "    # of the attention mechanism.\n",
        "memory = [q]\n",
        "# attends: A list of all tensors that represent what the network\n",
        "  attends to.\n",
        "attends = []\n",
        "for a in range(passes):\n",
        "        # attention mask\n",
        "attend_to = attention(cs, tf.tile(tf.reshape(memory[-1],[- 1,1,recurrent_cell_size]),size),\n",
        "facts_0s)\n",
        "# Inverse attention mask, for what's retained in the state.\n",
        "retain = 1-attend_to\n",
        "# GRU pass over the facts, according to the attention mask.\n",
        "while_valid_index = (lambda state, index: index < tf.shape(cs)[1])\n",
        "update_state = (lambda state, index: (attend_to[:,index,:] *\n",
        "attention_gru(cs[:,index,:], state)[0] +\n",
        "retain[:,index,:] *\n",
        "state))\n",
        "# start loop with most recent memory and at the first index memory.append(tuple(tf.while_loop(while_valid_index,\n",
        "(lambda state, index: (update_state(state,index),index+1)),\n",
        "loop_vars = [memory[-1], 0]))[0]) attends.append(attend_to)\n",
        "scope.reuse_variables()\n",
        " # Answer Module\n",
        "# a0: Final memory state\n",
        "a0 = tf.concat([memory[-1], q], -1)\n",
        "fc_init = tf.random_normal_initializer(stddev=0.1) with tf.variable_scope(\"answer\"):\n",
        "w_answer = tf.get_variable(\"weight\", [recurrent_cell_size*2, D], tf.float32, initializer = fc_init)\n",
        "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(w_answer))\n",
        "logit = tf.expand_dims(tf.matmul(a0, w_answer),1)\n",
        "68\n",
        "\n",
        "with tf.variable_scope(\"ending\"):\n",
        "all_ends = tf.reshape(input_sentence_endings, [-1,2]) range_ends = tf.range(tf.shape(all_ends)[0])\n",
        "ends_indices = tf.stack([all_ends[:,0],range_ends], axis=1) ind = tf.reduce_max(tf.scatter_nd(ends_indices, all_ends[:,1],\n",
        "[tf.shape(q)[0],\n",
        "tf.shape(all_ends)[0]]),\n",
        "axis=-1)\n",
        "range_ind = tf.range(tf.shape(ind)[0])\n",
        "mask_ends = tf.cast(tf.scatter_nd(tf.stack([ind, range_ind], axis=1),\n",
        " [tf.reduce_max(ind)+1, tf.shape(ind)[0]]), bool)\n",
        "tf.ones_like(range_ind),\n",
        " mask = tf.scan(tf.logical_xor,mask_ends, tf.ones_like(range_ind, dtype=bool))\n",
        "logits = - tf.reduce_sum(tf.square(context*tf.transpose(tf.expand_dims(\n",
        "tf.cast(mask, tf.float32),-1),[1,0,2]) - logit),\n",
        "axis=-1)\n",
        "# Training\n",
        "gold_standard = tf.placeholder(tf.float32, [None, 1, D], \"answer\") with tf.variable_scope('accuracy'):\n",
        "eq = tf.equal(context, gold_standard)\n",
        "corrbool = tf.reduce_all(eq,-1)\n",
        "logloc = tf.reduce_max(logits, -1, keep_dims = True)\n",
        "locs = tf.equal(logits, logloc)\n",
        "correctsbool = tf.reduce_any(tf.logical_and(locs, corrbool), -1)\n",
        "corrects = tf.where(correctsbool, tf.ones_like(correctsbool, dtype=tf.float32),\n",
        "tf.zeros_like(correctsbool,dtype=tf.float32))\n",
        "corr = tf.where(corrbool, tf.ones_like(corrbool, dtype=tf.float32), tf.zeros_like(corrbool,dtype=tf.float32))\n",
        "with tf.variable_scope(\"loss\"):\n",
        " # Use sigmoid cross entropy as the base loss.\n",
        "# Could try softmax cross entropy and see also.\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits =\n",
        "tf.nn.l2_normalize(logits,-1),labels = corr)\n",
        "# Add regularization losses, weighted by weight_decay.\n",
        "total_loss = tf.reduce_mean(loss) + weight_decay * tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
        "# Adam optimizor is chosen, which generally works well for most cases.\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "# Once we have an optimizer, we ask it to minimize the loss\n",
        "opt_op = optimizer.minimize(total_loss)\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "69\n",
        "\n",
        " # Launch the TensorFlow session\n",
        "sess = tf.Session() sess.run(init)\n",
        "def prep_batch(batch_data, more_data = False):\n",
        "context_vec, sentence_ends, questionvs, spt, context_words, cqas, answervs, _ = zip(*batch_data)\n",
        "ends = list(sentence_ends)\n",
        "maxend = max(map(len, ends))\n",
        "aends = np.zeros((len(ends), maxend)) for index, i in enumerate(ends):\n",
        "for indexj, x in enumerate(i): aends[index, indexj] = x-1\n",
        " new_ends = np.zeros(aends.shape+(2,))\n",
        "for index, x in np.ndenumerate(aends): new_ends[index+(0,)] = index[0] new_ends[index+(1,)] = x\n",
        "contexts = list(context_vec)\n",
        "max_context_length = max([len(x) for x in contexts]) contextsize = list(np.array(contexts[0]).shape) contextsize[0] = max_context_length\n",
        "final_contexts = np.zeros([len(contexts)]+contextsize)\n",
        "contexts = [np.array(x) for x in contexts] for i, context in enumerate(contexts):\n",
        "final_contexts[i,0:len(context),:] = context max_query_length = max(len(x) for x in questionvs) querysize = list(np.array(questionvs[0]).shape) querysize[:1] = [len(questionvs),max_query_length] queries = np.zeros(querysize)\n",
        "querylengths = np.array(list(zip(range(len(questionvs)),[len(q)-1 for q in questionvs])))\n",
        "questions = [np.array(q) for q in questionvs] for i, question in enumerate(questions):\n",
        "queries[i,0:len(question),:] = question\n",
        "data = {context_placeholder: final_contexts, input_sentence_endings:\n",
        "new_ends,\n",
        "query:queries, input_query_lengths:querylengths, gold_standard: answervs}\n",
        "return (data, context_words, cqas) if more_data else data\n",
        " # Prepare validation set\n",
        "batch = np.random.randint(final_test_data.shape[0], size=batch_size*10) batch_data = final_test_data[batch]\n",
        "validation_set, val_context_words, val_cqas = prep_batch(batch_data, True)\n",
        "def train(iterations, batch_size):\n",
        "training_iterations = range(0, iterations, batch_size) training_iterations = tqdm(training_iterations)\n",
        "accuracy_list = [] loss_list = [] epoc_list = []\n",
        "k=1\n",
        "wordz = []\n",
        "70\n",
        "\n",
        "for j in training_iterations:\n",
        " batch = np.random.randint(final_train_data.shape[0], size=batch_size)\n",
        "batch_data = final_train_data[batch] sess.run([opt_op], feed_dict=prep_batch(batch_data))\n",
        "if (j/batch_size) % display_step == 0:\n",
        "            # Calculate batch accuracy\n",
        "acc, ccs, tmp_loss, log, con, cor, loc = sess.run([corrects, cs, total_loss, logit, context_placeholder,corr, locs], feed_dict=validation_set)\n",
        "print(\"Iter \" + str(j/batch_size) + \", Minibatch Loss= \",\n",
        " tmp_loss, \"Accuracy= \", np.mean(acc)) accuracy_list.append(np.mean(acc))\n",
        "loss_list.append(tmp_loss) epoc_list.append(k)\n",
        "k=k+1\n",
        "import matplotlib.pyplot as plt plt.style.use('seaborn-white') plt.plot(epoc_list,accuracy_list, color='steelblue') plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy') plt.savefig('accuracy.png', dpi=300) plt.show()\n",
        "plt.plot(epoc_list,loss_list, color='steelblue') plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('loss.png', dpi=300)\n",
        "plt.show() train(training_iterations_count, batch_size)\n",
        "# Final testing accuracy\n",
        "print(\"Test Accuracy : \") print(np.mean(sess.run([corrects], feed_dict= prep_batch(final_test_data))[0]))\n",
        "ancr = sess.run([corrbool,locs, total_loss, logits, facts_0s, w_1]+attends+[query, cs,\n",
        " question_module_outputs],feed_dict=validation_set)\n",
        "a = ancr[0]\n",
        "n = ancr[1]\n",
        "cr = ancr[2]\n",
        "attenders = np.array(ancr[6:-3]) faq = np.sum(ancr[4], axis=(-1,-2))\n",
        "limit = 5\n",
        "for question in range(min(limit, batch_size)):\n",
        "plt.yticks(range(passes,0,-1))\n",
        "plt.ylabel(\"Episode\")\n",
        "plt.xlabel(\"Question \"+str(question+1))\n",
        "pltdata = attenders[:,question,:int(faq[question]),0] pltdata = (pltdata - pltdata.mean()) / ((pltdata.max() -\n",
        "pltdata.min() + 0.001)) * 256\n",
        "plt.pcolor(pltdata, cmap=plt.cm.Blues, alpha=0.7)\n",
        "71\n",
        "\n",
        "plt.show()\n",
        "# this get the predicted words location within the context.\n",
        "indices = np.argmax(n,axis=1)\n",
        "# Locations of actual answers within contexts\n",
        "indicesc = np.argmax(a,axis=1)\n",
        "for i,e,cw, cqa in list(zip(indices, indicesc, val_context_words, val_cqas))[:limit]:\n",
        "ccc = \" \".join(cw)\n",
        "print(\"TEXT: \",ccc)\n",
        "print (\"QUESTION: \", \" \".join(cqa[3]))\n",
        "print (\"RESPONSE: \", cw[i], [\"Correct\", \"Incorrect\"][i!=e])\n",
        "print(\"EXPECTED: \", cw[e]) print()\n",
        "from google.colab import files files.download('accuracy.png') files.download('loss.png')\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}